

# Machine Learning



[TOC]



## æ¨¡å‹åˆ†ç±»

- åˆ¤åˆ«æ¨¡å‹(discriminative model): é€šè¿‡æ¨¡å‹ç›´æ¥å¾—åˆ°ç±»åˆ«
- ç”Ÿæˆæ¨¡å‹(generative model): æ¯ä¸ªåˆ†ç±»ä¸€ä¸ªæ¦‚ç‡, é€‰æ¦‚ç‡é«˜çš„

> åˆ¤æ–­ä¸€ä¸ªåŠ¨ç‰©æ˜¯å¤§è±¡ï¼ˆy=1ï¼‰è¿˜æ˜¯å°ç‹—ï¼ˆy=0ï¼‰ã€‚
>
> - åˆ¤åˆ«æ¨¡å‹:
>
> > ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å¾—åˆ°åˆ¤åˆ«å¤§è±¡å’Œå°ç‹—çš„å†³ç­–é¢ï¼ˆdecision boundaryï¼‰ï¼Œæœ‰äº†è¿™ä¸ªå†³ç­–é¢ä¹‹åï¼Œå¦‚æœéœ€è¦åˆ†ç±»ä¸€ä¸ªæ–°åŠ¨ç‰©æ˜¯å¤§è±¡è¿˜æ˜¯å°ç‹—ï¼Œåªéœ€è¦åˆ¤æ–­å®ƒåœ¨å†³ç­–é¢çš„å“ªä¸€è¾¹ã€‚
>
> - ç”Ÿæˆæ¨¡å‹:
>
> > å…ˆè§‚å¯Ÿå¤§è±¡ï¼Œæ ¹æ®å…¶ç‰¹å¾å­¦ä¹ ä¸€ä¸ªå¤§è±¡çš„æ¨¡å‹ï¼ŒåŒæ ·çš„æˆ‘ä»¬å­¦ä¹ å¾—åˆ°ä¸€ä¸ªå°ç‹—çš„æ¨¡å‹ã€‚å½“æˆ‘ä»¬åˆ¤åˆ«ä¸€ä¸ªæ–°åŠ¨ç‰©æ—¶ï¼Œåˆ†åˆ«å’Œå¤§è±¡æ¨¡å‹å’Œå°ç‹—æ¨¡å‹æ¯”è¾ƒï¼Œå“ªä¸ªåŠ¨ç‰©æ›´åƒå°±è®¤ä¸ºæ–°åŠ¨ç‰©ä¸ºé‚£ä¸ªç±»ã€‚



## Liner Regression



### ä»£ä»·å‡½æ•°è§£æ³•

#### 1.  æœ€å°äºŒä¹˜æ³•(Least Squares => normal equation å³æ­£è§„æ–¹ç¨‹) (Analytic(aka closed form,è§£æè§£)solution)

![image-20181024113115695](assets/image-20181024113115695.png)

![image-20181024113126291](assets/image-20181024113126291.png)

#### Steps: 

![image-20181024114651460](assets/image-20181024114651460.png)



#### 2. æ¢¯åº¦ä¸‹é™ (Approximate iterative solution)



#### Steps:

![image-20181024114713775](assets/image-20181024114713775.png)







## Logistic Regression

- L1 norm: Manhattan Distance
- L2 norm: Euclidean distance

- Decision Boundary ( ä¸€èˆ¬æ˜¯ p(1|**x**) = 0.5æ—¶çš„ **x**æ‰€å¯¹åº”çš„boundaryæ¥åŒºåˆ† )

| ![image-20181024134207814](assets/image-20181024134207814.png) | ![image-20181024134220852](assets/image-20181024134220852.png) (åœ†ç‚¹åœ¨åŸç‚¹ä¸”åŠå¾„ä¸º1çš„åœ†å½¢) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20181024134134910](assets/image-20181024134134910.png) | ![image-20181024134144721](assets/image-20181024134144721.png) |



|                   |                                                              |
| ----------------- | ------------------------------------------------------------ |
| Original Equation | ![image-20181024125455834](assets/image-20181024125455834.png) |
| Hence             | ![image-20181024125618022](assets/image-20181024125618022.png) |
|                   |                                                              |
|                   |                                                              |





#### Training as Max Likelihood Estimation

##### Iterative optimisation

- Bad news: **No closed form solution(è§£æè§£)**
- Good news: Problem is **strictly convex** (like a bowl, å‡¸å‡½æ•°) if there are no irrelevant features --> 
  optimisation guaranteed to work! (using **regularisatoin** to deal with irrelevant features)



#### Training as crossâ€entropy minimisation

| for a single data point                                      | for data set                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20181024134509101](assets/image-20181024134509101.png)<br />å³<br />![image-20181024134932595](assets/image-20181024134932595.png) | ![image-20181024135154861](assets/image-20181024135154861.png) |





## Basis expansion

### Marry non-liner data to a liner method

#### Transform the data (map data onto another features space)



###  Radial basis functions(RBFs)

A **radial basis function** is a function of the form 

![image-20181024140625102](assets/image-20181024140625102.png)

Where z is a constant



### Challenges 

- the transformation needs to be defined beforehand
  - can choose uniformly spaced points **or** cluster training data and use cluster centroids
  - popular idea is to use training data $z_i \equiv x_i $
    - results in a large number of features



### How to improve (futher directions)

- learn transformation function from data (ANN)
- kernel trick(kernelised methods)
- sparse kernel machines (SVM, training depends only on a few data points)





## Regularisation

ç”¨æ¥è§£å†³ill-posed(ç»“æœä¸å”¯ä¸€/ä¸ç¨³å®š ==> inputå¹²æ‰°å¯¼è‡´ç»“æœå˜åŒ–å‰§çƒˆ)å’Œè¿‡æ‹Ÿåˆ(overfitting)



![image-20181024142252225](assets/image-20181024142252225.png)

- $\lambda$    è¢«ç§°ä¸º regularization parameter



### è¦è§£å†³çš„é—®é¢˜

**overfit**



#### ä¸ç›¸å…³feature (Irrelevant features)

å½“feature $X_j$ æ˜¯æ˜¯å…¶ä»–featuresçš„æŸç§å‡½æ•°å¾—åˆ°çš„, é‚£ä¹ˆå°±feature $X_j$ å°±æ˜¯irrelevant feature

![image-20181024150327941](assets/image-20181024150327941.png)



**æ•°æ®ç¼ºå¤± (Lack of Data)**

ä¾‹å­æ˜¯Modelå‚æ•°æ¯”data instances countè¿˜å¤š





#### ill-posed problems

- ç»“æœä¸å”¯ä¸€/ä¸ç¨³å®š ==> inputå¹²æ‰°å¯¼è‡´ç»“æœå˜åŒ–å‰§çƒˆ
- **ä¾‹å­**: æ­£è§„æ–¹ç¨‹æ±‚ $\theta$ çš„æ—¶å€™,  $X'X$æ²¡æœ‰é€†



 ### Solution - Regularisation

#### ä½¿ç”¨regularisation, å¼•å…¥**additional condition**

| Original Loss Function (maximum likehood estimation, MLE)    | After regularisation (maximum a posteriori, MAP)             |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20181024152021749](assets/image-20181024152021749.png) | ![image-20181024152013780](assets/image-20181024152013780.png) |

æ­¤æ—¶æ­£è§„æ–¹ç¨‹è§£æ³•çš„å…¬å¼ä¸º **(ridge regression - å²­å›å½’)**:

![image-20181024152132836](assets/image-20181024152132836.png)

#### æ¨å¯¼: 

Slide 05. p16



### å¦‚ä½•é€‰æ‹©model

#### Explicit model selection

- å°è¯•
- æµ‹è¯•(evaluate)
- æ¯”è¾ƒ
- é€‰æ‹©



#### Vary complexity by regularisation

- è½¬åŒ–é—®é¢˜(augment the problem, by åŠ å…¥regularisation factor), ä¾‹å¦‚ä¸Šé¢æåˆ°çš„ridge regression(å²­å›å½’)

![image-20181024154219638](assets/image-20181024154219638.png)

- æ ¹æ®$R$ è®¡ç®—/evaluate/compare å¹¶æœ€ç»ˆé€‰æ‹© $\lambda$



### ä¸åŒRegularisedæ–¹å¼çš„ Liner Regression å¯¹æ¯”

![image-20181024154917930](assets/image-20181024154917930.png)





## Bias-variance trade-off

### ä»€ä¹ˆæ˜¯Biaså’Œvariance

##### - Bias: åœ¨æ ·æœ¬ä¸Šæ‹Ÿåˆçš„å¥½ä¸å¥½

##### - Variance: æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å¥½ä¸å¥½

![image-20181024161159494](assets/image-20181024161159494.png)

- complexity model

  - è´Ÿè´£çš„modelé€šå¸¸å¯ä»¥è®© training errorå˜å¾—å¾ˆå°
  - ç”šè‡³æ˜¯0 error(æœ‰é™sampleçš„æ—¶å€™)

  |               |           |               |
  | ------------- | --------- | ------------- |
  | simple model  | High bias | Low variance  |
  | complex model | Low bias  | High variance |


### bias, varianceå’Œtest errorä»¥åŠunderfit/overfitçš„å…³ç³»

|                                                              |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20181024161412052](assets/image-20181024161412052.png) | ![image-20181024161422765](assets/image-20181024161422765.png) |



## Perceptron (æ„ŸçŸ¥æœº, ç¥ç»ç½‘ç»œåŸºç¡€)

- activation function
- feed forward networks

#### Convergence Theorem: 

```
if the training data is linearly separable, the algorithm is guaranteed to converge to a solution. That is, there exist a finite ğ¾ such that L(w^K) = 0
```



#### Update $w$  in training

![image-20181024230809722](assets/image-20181024230809722.png) 



## Multilayer Perceptron 



#### Activation Funciton

![image-20181025194005547](assets/image-20181025194005547.png)



#### Loss funciton

![image-20181025194020289](assets/image-20181025194020289.png)

- differentiable
- no analytic solution (é€šå¸¸æ²¡æœ‰è§£æè§£)
- ä½¿ç”¨stochastic gradient descentç®—(éšæœºæ¢¯åº¦ä¸‹é™, æ¯æ¬¡æ›´æ–°$\theta$åªç”¨ä¸€ä¸ªsample) - å¯¹æ¯”æ‰¹é‡æ¢¯åº¦ä¸‹é™, æ¯æ¬¡æ›´æ–°ä½¿ç”¨å…¨éƒ¨sample







## Backpropagation ( of errors)

- ä»æœ€åä¸€å±‚å‘å‰(åå¯¼)
- æ­£åˆ™åŒ–(regularisation, implicit and explicit)





## Deep learning

- Any Boolean function over ğ‘š variables can be implemented using a hidden layer with up to 2ô° elements 
- **vanishing gradient problem**

> **åŸå› ï¼š**å‰é¢å±‚ä¸Šçš„æ¢¯åº¦æ˜¯æ¥è‡ªäºåé¢å±‚ä¸Šæ¢¯åº¦çš„ä¹˜ä¹˜ç§¯ã€‚å½“å­˜åœ¨è¿‡å¤šçš„å±‚æ¬¡æ—¶ï¼Œå°±å‡ºç°äº†å†…åœ¨æœ¬è´¨ä¸Šçš„ä¸ç¨³å®šåœºæ™¯ï¼Œå¦‚æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸ã€‚
>
> https://blog.csdn.net/cppjava_/article/details/68941436





### Convolutional Neural Networks(CNN)

### Components of a CNN

- Convolutional layers
  - complex input representations based on convolution operation
  - filter weights are learned from training data
- **Downsampling (usually via Max Pooling)**
  - re-scales to smaller resolution, limits parameter explosion
  - Max Pooling: 
    - $v = max(u_{11},u_{11},...,u_{mm})$ (é€‰æ‹©å·ç§¯ä¸­çš„æœ€å¤§valuableçš„é‚£ä¸ª)
    - invariance(ä¸å˜æ€§)ï¼Œè¿™ç§ä¸å˜æ€§åŒ…æ‹¬translation(å¹³ç§»)ï¼Œrotation(æ—‹è½¬)ï¼Œscale(å°ºåº¦)
    - ä¿ç•™ä¸»è¦çš„ç‰¹å¾åŒæ—¶å‡å°‘å‚æ•°(é™ç»´ï¼Œæ•ˆæœç±»ä¼¼PCA)å’Œè®¡ç®—é‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›
    - https://www.zhihu.com/question/36686900
- Fully connected parts and output layer
  - merges representations together



## Support Vector Machines

https://zhuanlan.zhihu.com/p/31258516

![image-20181026120509487](assets/image-20181026120509487.png)

### Difference between SVM and perceptron

- Perceptron: min perceptron loss as studied earlier
- SVM: different criterion for choosing parameters



### Maximum margin classifier

- perpendicular to(å‚ç›´äº)

- æœ€ä¼˜boundaryæœ‰æ— é™ä¸ª(æœ‰æ­§ä¹‰çš„, ambiguity, å½“ **$w$** å’Œ$b$éƒ½æ˜¯æ•´æ•°å€, å®é™…ä¸Šæ˜¯ä¸€æ¡çº¿,ä½†æ˜¯æœ‰æ— æ•°ç§è¡¨ç¤ºæ–¹æ³•)

  - è§£å†³æ–¹æ¡ˆ1: measure the distance to the closest point, and rescale parameters(ä»è€Œä½¿è§£å˜å¾—å”¯ä¸€):

  > **Steps:**
  >
  > ![image-20181026120550200](assets/image-20181026120550200.png)


### hard-margin SVM

![image-20181026120730050](assets/image-20181026120730050.png)



### Soft-margin SVM

3 approaches to address ***not-separable*** problem

- transform data (still use hard-margin svm)
- relax the constraints
- combination of above 2 approaches



#### Relax the contrains: allow points to be inside the margin or even on the wrong side of the boundary (ä½†æ˜¯ä¼šåŠ å…¥â€œæƒ©ç½šâ€æœºåˆ¶)



| Hard Margin                                                  | Soft Margin                                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![ximage-20181026131313831](assets/image-20181026131313831.png) | ![image-20181026131332246](assets/image-20181026131332246.png) |
| å…¶å®å°±æ˜¯soft marginä¸­æ­£åˆ™åŒ–å¸¸æ•°Cçš„å€’æ•°($\frac{1}{\lambda}$)æ— é™å¤§, ä½¿å¾—å…¶ç¬¬äºŒé¡¹å¯¹é”™è¯¯åˆ†ç±»çš„å®¹å¿åº¦å˜å¾—æå°(æ—¢ä¸å…è®¸é”™è¯¯), å®¹æ˜“å‡ºç°**overfit** | Cè¶Šå°, å¯¹é”™è¯¯çš„å®¹å¿åº¦è¶Šå¤§(æ—¢å…è®¸é”™è¯¯), å®¹æ˜“å‡ºç°**underfit    |
|                                                              |                                                              |



### è§£æ³•

- æ— çº¦æŸçš„ä¼˜åŒ–é—®é¢˜: $min f(x)$
- å¸¦ç­‰å¼çº¦æŸçš„ä¼˜åŒ–é—®é¢˜:  $minf(x), s.t. h(x)=0$  (Lagrangian Duality)
- å¸¦ä¸ç­‰å¼çº¦æŸçš„ä¼˜åŒ–é—®é¢˜: $min f(x) , s.t.h(x) \leq 0$ (KKT )



### æ ¸å‡½æ•° (for non-liner data, feature space transformation)

- Map data into a new feature space
- run hard-margin or soft-margin SVM in new space
- decision boundary is non-linear in original space



#### é«˜æ–¯æ ¸å‡½æ•°

- åœ°æ ‡ (landmarks)
- **Gaussian Kernel**
  - è¾ƒå¤§æ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä½æ–¹å·®ï¼Œé«˜åå·®(bias, underfit)ï¼›
  - è¾ƒå°æ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä½åå·®ï¼Œé«˜æ–¹å·®(variance, overfit)

> ![image-20181026160456787](assets/image-20181026160456787.png)

- å¤šé¡¹å¼æ ¸å‡½æ•°ï¼ˆ**Polynomial Kerne**lï¼‰
- å­—ç¬¦ä¸²æ ¸å‡½æ•°ï¼ˆ**String kernel**ï¼‰
- å¡æ–¹æ ¸å‡½æ•°ï¼ˆ **chi-square kernel**ï¼‰
- ç›´æ–¹å›¾äº¤é›†æ ¸å‡½æ•°ï¼ˆ**histogram intersection kernel**ï¼‰



### Logical regression vs SVM

$n$ä¸ºç‰¹å¾æ•°ï¼Œ$m$ä¸ºè®­ç»ƒæ ·æœ¬æ•°ã€‚

- $n > m$   ==> LR or SVM without kernel
- small n(1-1000) and not that big m(10-10000)  ==> SVM with Gaussian kernel
- small n and big m  ==> æ‰¾åˆ°æ›´å¤šfeatures, å¹¶ä½¿ç”¨ç¬¬ä¸€ç§





### Representer theorem

ä¸€ä¸ªç”¨æ¥è¯†åˆ«æ–°æ–¹æ³•(learner)æ˜¯ä¸æ˜¯æœ‰æ•ˆçš„æ ¸å‡½æ•°çš„tool

**(Tells us when a (decisionâ€theoretic) learner is kernelizable)**

- $f$ function is a reproducing kernel Hilbert space(RKHS)



## Constructing Kernels

### Polynomial kernel

![image-20181026171845522](assets/image-20181026171845522.png)

- Here ğ’– and ğ’— are vectors with ğ‘š components 
- ğ‘‘ ô°† 0isanintegerandğ‘ ô°† 0isaconstant 



### Identifying new kernels

#### Method 01

![image-20181026172150552](assets/image-20181026172150552.png)



#### Method 02

Using **Mercerâ€™s** theorem

using function $f$ to generate a matrix of $n * n$ size (each item = $f(a_i,a_j)$) and if the matrix is **positive-semidefinite**, then it is a valid kernel function.

![image-20181026173402324](assets/image-20181026173402324.png)







Example: è¯æ˜Gaussian Kernelæ˜¯ä¸€ä¸ªæ ¸å‡½æ•°



![image-20181026172857086](assets/image-20181026172857086.png)

> æœ€åä¸€æ­¥çš„è¯´æ˜(æ ¹æ®method 01çš„identity):
>
> - ç‚¹ä¹˜æ˜¯æ ¸å‡½æ•°(dæ¬¡æ–¹çš„dot product)
> - æ ¸å‡½æ•°ä¹˜ä»¥å¸¸æ•°($r_d$)è¿˜æ˜¯æ ¸å‡½æ•°
> - æ ¸å‡½æ•°ç›¸åŠ è¿˜æ˜¯æ ¸å‡½æ•°
> - æ ¸å‡½æ•°å‰åä¹˜ä»¥$f(u), f(v)$ä»ç„¶æ˜¯æ ¸å‡½æ•°
>
> ===> æ‰€ä»¥æ•´ä¸ªä¸­é—´é¡¹æ˜¯æ ¸å‡½æ•°





## Ensemble Methods (é›†æˆå­¦ä¹ )

###  Bagginng

- Construct "near-independent" datasets via sampling with replacement(æœ‰æ”¾å›çš„é‡å¤æŠ½æ ·, æ¯æ¬¡å–æ ·åæ ·å“ä¸ä¼šè¢«drop, ä»ç„¶åœ¨å–æ ·ç©ºé—´ä¸­)
- sampling - training - predicting - evalucating 
- å¯å¹¶è¡Œ
- è§£å†³high variance(overfit)çš„é—®é¢˜



#### Sampling

| Original training set                                        | Samples (with replacement)                                   |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20181026191540608](assets/image-20181026191540608.png) | ![image-20181026191532759](assets/image-20181026191532759.png) |

#### Example: Random Forest (baggingçš„å…¸å‹ç®—æ³•)

![image-20181026191937996](assets/image-20181026191937996.png)

> æ¯æ£µæ ‘çš„featureséƒ½æ˜¯ä¸åŒçš„ (ä¹Ÿæœ‰è¯´æ¯ä¸ªnodeçš„featureéƒ½æ˜¯ä¸åŒçš„)

 ##### ä½¿ç”¨out-of-sample dataè¿›è¡Œæµ‹è¯•

> ç†è®ºä¸Š, å½“æ ·æœ¬è¶³å¤Ÿå¤šçš„æ—¶å€™, æ¯æ¡æ ·æœ¬éƒ½æœ‰$e^{-1}=0.368$çš„æ¦‚ç‡ä¸è¢«é€‰åˆ°.
>
> è¿™éƒ¨åˆ†æ•°æ®å«åš out-of-sample data, å¯ä»¥ç”¨æ¥åševaluate



### Boosting

- è§£å†³high bias(underfit)çš„é—®é¢˜
- æœ‰æ”¾å›çš„æŠ½æ ·(ä½†æ˜¯åŸºäºweighté€‰æ‹©, ä¸æ˜¯random)
- é€‚ç”¨äºåˆ†ç±»å™¨æ˜¯â€œ**å¼±(weak)åˆ†ç±»å™¨**â€(å®¹æ˜“underfit)
- åŸºäºè¿­ä»£(iteration), æ¯æ¬¡è¿­ä»£é€‰æ‹©æ ·æœ¬çš„æ—¶å€™,åŸºäºä¹‹å‰è¿­ä»£ç»“æœå¾—åˆ°çš„**æƒé‡(weight)**
- è®¡ç®—é‡æ¯”baggingå¤§
- å®¹æ˜“overfit (æ‰€ä»¥åªé€‚ç”¨äºå¼±åˆ†ç±»å™¨, ä¹Ÿå°±æ˜¯å®¹æ˜“underfitçš„åˆ†ç±»å™¨; å¦‚æœä½¿ç”¨performanceè¾ƒå¥½çš„åŸºåˆ†ç±»å™¨å°±æ›´å®¹æ˜“overfit)





#### Sampling

| Original                                                     | Boosting (more likely to select the ones that are misclassified) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20181026213427538](assets/image-20181026213427538.png) | ![image-20181026213433486](assets/image-20181026213433486.png) |



#### Steps

![image-20181026213315303](assets/image-20181026213315303.png)


#### Example: AdaBoost

![image-20181026213556092](assets/image-20181026213556092.png)





#### Bagging vs Boosting

![image-20181026214050643](assets/image-20181026214050643.png)



#### Stacking

æœ‰ç‚¹åƒANN, æ ¹æ®base modelçš„outputå†æ¬¡ç”Ÿæˆmeta-model,ç±»ä¼¼äºç»™æ¯ä¸ªbase modelä¸€ä¸ªæƒé‡(linear regression)

 $meta\_model=f(base\_models)$

- è®¡ç®—é‡å¤§ (computationally expensive)
- å¯ä»¥ä½¿ç”¨å¤šç§base model



## Bandit

### MAB(Multi-armed bandit)

##### $\epsilon-Greedy $

- use $\epsilon $(0~1) to control exploration and exploitation
- trade-off between exploration and exploitation (using $\epsilon$) 
  - greedy($\epsilon=0$): increases fastest
  - high exploration (high $\epsilon$): increases faster
  - high exploitation (low $\epsilon$): increases slower, but eventually superior to high $\epsilon$.
- selection of initialisation value for Greedy ($\epsilon = 0$) (of estimate value Q)
  - Pessimism(æ‚²è§‚): ä½¿ç”¨æ¯”è§‚å¯Ÿå€¼å°çš„ --> æ°¸è¿œåªé€‰ä¸€ä¸ªarm 
  - Optimism(ä¹è§‚): ä½¿ç”¨æ¯”è§‚å¯Ÿå€¼å¤§çš„ --> å°è¯•æ‰€æœ‰arms



##### UCB (Upper Confidence Bandit)

UCB algorithm

|                                                              | Average Reward                                               | regretbounds                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20181027105112615](assets/image-20181027105112615.png) | ![image-20181027105120356](assets/image-20181027105120356.png) | ![image-20181027105131296](assets/image-20181027105131296.png) |
|                                                              | ![image-20181027105152717](assets/image-20181027105152717.png)<br />å‰t-1è½®çš„å¹³å‡reward | ![image-20181027105238903](assets/image-20181027105238903.png)<br /><br />è¿™ä¸€æ•´é¡¹ä»£è¡¨ä¸€ä¸ªåŠ æƒ, è¢«é€‰æ‹©æ¬¡æ•°è¶Šå°‘çš„armè¿™ä¸€é¡¹å°±è¶Šå¤§, ä¹Ÿå°±è¶Šæœ‰å¯èƒ½è¢«explore.<br />ä¹Ÿå°±æ˜¯è¯´è¿™ä¸€é¡¹æ˜¯ç”¨æ¥å¹³è¡¡explorationå’Œexploitationçš„ |

###### Flexible change

Using $p$ to replace $2$ in the second part for the reason of balancing the weight of **selecting existing best one** or **exploration**.

![image-20181027105639053](assets/image-20181027105639053.png)



## Unsupervised Learning

### Tasks

- Clustering
- Dimensionality reduction
- Learning parameters of probabilistic models

### Applications:

- å•†å“å…³è”æ€§åˆ†æ(ç‰©å“æ€»æ˜¯è¢«ä¸€èµ·ä¹°, å•¤é…’å°¿å¸ƒ)
- å¥‡å¼‚å€¼æ£€æµ‹(è¯ˆéª—è¯†åˆ«, ä¿¡ç”¨å¡ç›—åˆ·è¯†åˆ«)



### K-means clustering

![image-20181027110900276](assets/image-20181027110900276.png)



## Gaussian Mixture Model (GMM)

clusteringé—®é¢˜è½¬åŒ–

![image-20181027115321582](assets/image-20181027115321582.png)

- æ— æ³•æ±‚å¯¼
- log trickåæ±‚å¯¼ä¹Ÿä¸ä¿¡
- åå¯¼ä¹Ÿä¸è¡Œ, å› æ­¤æ¢¯åº¦ä¸‹é™ä¸è¡Œ
- **åªèƒ½ç”¨Expectation Maximisation(EM)**



### Expectation Maximisation

| MLE                                                  | EM                                      |
| ---------------------------------------------------- | --------------------------------------- |
| a frequentist principle                              | an algorithm(æ±‚è§£MLEçš„)                 |
| maximise the probability of the data                 | a way to solve the problem posed by MLE |
| can be solved by other methods like gradient descent |                                         |
|                                                      |                                         |

#### ä¸ºä»€ä¹ˆè¦ç”¨EMç®—æ³•

1. æ²¡æœ‰è¶³å¤Ÿçš„å·²çŸ¥é‡æ¥è®¡ç®—
2. è®¡ç®—å¾ˆéº»çƒ¦



#### EMçš„ä¸»è¦æ€æƒ³

- å°†å˜é‡åˆ†ä¸º$observed$  variableså’Œ*unobserved* variables (latent variables)

- åŠ å…¥é¢å¤–çš„å†—ä½™å˜é‡(additional variables might seem redundant)

- maximising ä¸‹é™

  ![image-20181027120922034](assets/image-20181027120922034.png)

> **key idea: ä½¿ç”¨$\theta$ æ¥è¡¨ç¤º$p(Z)$**
>
> å³è¾¹éƒ¨åˆ†å°±æ˜¯EMçš„ä¸‹é™. æé«˜ä¸‹é™ä¹Ÿå°±æ˜¯æé«˜å³è¾¹éƒ¨åˆ†çš„å€¼.
>
> - 0. å³è¾¹éƒ¨åˆ†æ˜¯ä¸€ä¸ªå…³äº**ä¸¤ä¸ª**å˜é‡çš„å‡½æ•°, æ— æ³•åŒæ—¶æ±‚è§£, å› æ­¤ä½¿ç”¨**æ§åˆ¶å˜é‡** (å…ˆå›ºå®š$\theta$ç®—$p(Z)$, åœ¨å›ºå®š$p(Z)$ç®—$\theta$).
>
> - 1. å½“set $p(Z) = p(Z|X,\theta^*)$æ—¶, lower bound becomes tight 
> - 2. lower boundçš„ç¬¬äºŒé¡¹æ˜¯ä¸$\theta$æ— å…³çš„
> - 3. æ»¡è¶³step1,2, lower boundå°±æ˜¯ä¸€ä¸ªå…³äº$\theta$çš„ä¸€å…ƒå‡½æ•°, å°±å¯ä»¥æ±‚è§£æè§£äº†



### GMMä¸­çš„EM

Step E (Expectation)



Step M (Maximisation)



#### responsibilities

![image-20181028105044102](assets/image-20181028105044102.png)



#### K-meansæ˜¯ä¸€ç§ç‰¹æ®Šçš„GMM(æ¡ä»¶å—é™)

k-meansæ˜¯æ¯ä¸ªé«˜æ–¯åˆ†å¸ƒçš„æƒé‡éƒ½æ˜¯$\frac{1}{k}$ çš„GMM







## PCA

reduce count of dimensions via data transformation

![image-20181028123910018](assets/image-20181028123910018.png)



### åº”ç”¨åœºæ™¯

å‡ä½¿æˆ‘ä»¬æ­£åœ¨é’ˆå¯¹ä¸€å¼  100Ã—100åƒç´ çš„å›¾ç‰‡è¿›è¡ŒæŸä¸ªè®¡ç®—æœºè§†è§‰çš„æœºå™¨å­¦ä¹ ï¼Œå³æ€»å…±æœ‰10000 ä¸ªç‰¹å¾ã€‚

> 1. ç¬¬ä¸€æ­¥æ˜¯è¿ç”¨ä¸»è¦æˆåˆ†åˆ†æå°†æ•°æ®å‹ç¼©è‡³1000ä¸ªç‰¹å¾
> 2. ç„¶åå¯¹è®­ç»ƒé›†è¿è¡Œå­¦ä¹ ç®—æ³•
> 3. åœ¨é¢„æµ‹æ—¶ï¼Œé‡‡ç”¨ä¹‹å‰å­¦ä¹ è€Œæ¥çš„å°†è¾“å…¥çš„ç‰¹å¾è½¬æ¢æˆç‰¹å¾å‘é‡ï¼Œç„¶åå†è¿›è¡Œé¢„æµ‹
>
> æ³¨ï¼šå¦‚æœæˆ‘ä»¬æœ‰äº¤å‰éªŒè¯é›†åˆæµ‹è¯•é›†ï¼Œä¹Ÿé‡‡ç”¨å¯¹è®­ç»ƒé›†å­¦ä¹ è€Œæ¥çš„ã€‚



#### é”™è¯¯ç”¨æ³•

- ç”¨äºå‡å°‘è¿‡æ‹Ÿåˆ
- æ»¥ç”¨ (å°è¯•ç®—æ³•ä¹‹å‰æ€»æ˜¯å…ˆè¿›è¡ŒPCA, è¿™å¯èƒ½ä¼šä¸¢æ‰ä¸€äº›é‡è¦çš„feature)



## BayesianRegression

1. å‡è®¾å…ˆéªŒæ¦‚ç‡ prior
2. è®­ç»ƒè¿‡ç¨‹ä¸­å¾—åˆ°åéªŒæ¦‚ç‡ posterior



















