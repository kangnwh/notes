

# Machine Learning



[TOC]



## Ê®°ÂûãÂàÜÁ±ª

- Âà§Âà´Ê®°Âûã(discriminative model): ÈÄöËøáÊ®°ÂûãÁõ¥Êé•ÂæóÂà∞Á±ªÂà´
- ÁîüÊàêÊ®°Âûã(generative model): ÊØè‰∏™ÂàÜÁ±ª‰∏Ä‰∏™Ê¶ÇÁéá, ÈÄâÊ¶ÇÁéáÈ´òÁöÑ

> Âà§Êñ≠‰∏Ä‰∏™Âä®Áâ©ÊòØÂ§ßË±°Ôºày=1ÔºâËøòÊòØÂ∞èÁãóÔºày=0Ôºâ„ÄÇ
>
> - Âà§Âà´Ê®°Âûã:
>
> > ‰ªéËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Â≠¶‰π†ÂæóÂà∞Âà§Âà´Â§ßË±°ÂíåÂ∞èÁãóÁöÑÂÜ≥Á≠ñÈù¢Ôºàdecision boundaryÔºâÔºåÊúâ‰∫ÜËøô‰∏™ÂÜ≥Á≠ñÈù¢‰πãÂêéÔºåÂ¶ÇÊûúÈúÄË¶ÅÂàÜÁ±ª‰∏Ä‰∏™Êñ∞Âä®Áâ©ÊòØÂ§ßË±°ËøòÊòØÂ∞èÁãóÔºåÂè™ÈúÄË¶ÅÂà§Êñ≠ÂÆÉÂú®ÂÜ≥Á≠ñÈù¢ÁöÑÂì™‰∏ÄËæπ„ÄÇ
>
> - ÁîüÊàêÊ®°Âûã:
>
> > ÂÖàËßÇÂØüÂ§ßË±°ÔºåÊ†πÊçÆÂÖ∂ÁâπÂæÅÂ≠¶‰π†‰∏Ä‰∏™Â§ßË±°ÁöÑÊ®°ÂûãÔºåÂêåÊ†∑ÁöÑÊàë‰ª¨Â≠¶‰π†ÂæóÂà∞‰∏Ä‰∏™Â∞èÁãóÁöÑÊ®°Âûã„ÄÇÂΩìÊàë‰ª¨Âà§Âà´‰∏Ä‰∏™Êñ∞Âä®Áâ©Êó∂ÔºåÂàÜÂà´ÂíåÂ§ßË±°Ê®°ÂûãÂíåÂ∞èÁãóÊ®°ÂûãÊØîËæÉÔºåÂì™‰∏™Âä®Áâ©Êõ¥ÂÉèÂ∞±ËÆ§‰∏∫Êñ∞Âä®Áâ©‰∏∫ÈÇ£‰∏™Á±ª„ÄÇ



## Liner Regression



### ‰ª£‰ª∑ÂáΩÊï∞Ëß£Ê≥ï

#### 1.  ÊúÄÂ∞è‰∫å‰πòÊ≥ï(Least Squares => normal equation Âç≥Ê≠£ËßÑÊñπÁ®ã) (Analytic(aka closed form,Ëß£ÊûêËß£)solution)

![image-20181024113115695](assets/image-20181024113115695.png)

![image-20181024113126291](assets/image-20181024113126291.png)

#### Steps: 

![image-20181024114651460](assets/image-20181024114651460.png)



#### 2. Ê¢ØÂ∫¶‰∏ãÈôç (Approximate iterative solution)



#### Steps:

![image-20181024114713775](assets/image-20181024114713775.png)







## Logistic Regression

- L1 norm: Manhattan Distance
- L2 norm: Euclidean distance

- Decision Boundary ( ‰∏ÄËà¨ÊòØ p(1|**x**) = 0.5Êó∂ÁöÑ **x**ÊâÄÂØπÂ∫îÁöÑboundaryÊù•Âå∫ÂàÜ )

| ![image-20181024134207814](assets/image-20181024134207814.png) | ![image-20181024134220852](assets/image-20181024134220852.png) (ÂúÜÁÇπÂú®ÂéüÁÇπ‰∏îÂçäÂæÑ‰∏∫1ÁöÑÂúÜÂΩ¢) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20181024134134910](assets/image-20181024134134910.png) | ![image-20181024134144721](assets/image-20181024134144721.png) |



|                   |                                                              |
| ----------------- | ------------------------------------------------------------ |
| Original Equation | ![image-20181024125455834](assets/image-20181024125455834.png) |
| Hence             | ![image-20181024125618022](assets/image-20181024125618022.png) |
|                   |                                                              |
|                   |                                                              |





#### Training as Max Likelihood Estimation

##### Iterative optimisation

- Bad news: **No closed form solution(Ëß£ÊûêËß£)**
- Good news: Problem is **strictly convex** (like a bowl, Âá∏ÂáΩÊï∞) if there are no irrelevant features --> 
  optimisation guaranteed to work! (using **regularisatoin** to deal with irrelevant features)



#### Training as cross‚Äêentropy minimisation

| for a single data point                                      | for data set                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20181024134509101](assets/image-20181024134509101.png)<br />Âç≥<br />![image-20181024134932595](assets/image-20181024134932595.png) | ![image-20181024135154861](assets/image-20181024135154861.png) |





## Basis expansion

### Marry non-liner data to a liner method

#### Transform the data (map data onto another features space)



###  Radial basis functions(RBFs)

A **radial basis function** is a function of the form 

![image-20181024140625102](assets/image-20181024140625102.png)

Where z is a constant



### Challenges 

- the transformation needs to be defined beforehand
  - can choose uniformly spaced points **or** cluster training data and use cluster centroids
  - popular idea is to use training data $z_i \equiv x_i $
    - results in a large number of features



### How to improve (futher directions)

- learn transformation function from data (ANN)
- kernel trick(kernelised methods)
- sparse kernel machines (SVM, training depends only on a few data points)





## Regularisation

Áî®Êù•Ëß£ÂÜ≥ill-posed(ÁªìÊûú‰∏çÂîØ‰∏Ä/‰∏çÁ®≥ÂÆö ==> inputÂπ≤Êâ∞ÂØºËá¥ÁªìÊûúÂèòÂåñÂâßÁÉà)ÂíåËøáÊãüÂêà(overfitting)



![image-20181024142252225](assets/image-20181024142252225.png)

- $\lambda$    Ë¢´Áß∞‰∏∫ regularization parameter



### Ë¶ÅËß£ÂÜ≥ÁöÑÈóÆÈ¢ò

**overfit**



#### ‰∏çÁõ∏ÂÖ≥feature (Irrelevant features)

ÂΩìfeature $X_j$ ÊòØÊòØÂÖ∂‰ªñfeaturesÁöÑÊüêÁßçÂáΩÊï∞ÂæóÂà∞ÁöÑ, ÈÇ£‰πàÂ∞±feature $X_j$ Â∞±ÊòØirrelevant feature

![image-20181024150327941](assets/image-20181024150327941.png)



**Êï∞ÊçÆÁº∫Â§± (Lack of Data)**

‰æãÂ≠êÊòØModelÂèÇÊï∞ÊØîdata instances countËøòÂ§ö





#### ill-posed problems

- ÁªìÊûú‰∏çÂîØ‰∏Ä/‰∏çÁ®≥ÂÆö ==> inputÂπ≤Êâ∞ÂØºËá¥ÁªìÊûúÂèòÂåñÂâßÁÉà
- **‰æãÂ≠ê**: Ê≠£ËßÑÊñπÁ®ãÊ±Ç $\theta$ ÁöÑÊó∂ÂÄô,  $X'X$Ê≤°ÊúâÈÄÜ



 ### Solution - Regularisation

#### ‰ΩøÁî®regularisation, ÂºïÂÖ•**additional condition**

| Original Loss Function (maximum likehood estimation, MLE)    | After regularisation (maximum a posteriori, MAP)             |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20181024152021749](assets/image-20181024152021749.png) | ![image-20181024152013780](assets/image-20181024152013780.png) |

Ê≠§Êó∂Ê≠£ËßÑÊñπÁ®ãËß£Ê≥ïÁöÑÂÖ¨Âºè‰∏∫ **(ridge regression - Â≤≠ÂõûÂΩí)**:

![image-20181024152132836](assets/image-20181024152132836.png)

#### Êé®ÂØº: 

Slide 05. p16



### Â¶Ç‰ΩïÈÄâÊã©model

#### Explicit model selection

- Â∞ùËØï
- ÊµãËØï(evaluate)
- ÊØîËæÉ
- ÈÄâÊã©



#### Vary complexity by regularisation

- ËΩ¨ÂåñÈóÆÈ¢ò(augment the problem, by Âä†ÂÖ•regularisation factor), ‰æãÂ¶Ç‰∏äÈù¢ÊèêÂà∞ÁöÑridge regression(Â≤≠ÂõûÂΩí)

![image-20181024154219638](assets/image-20181024154219638.png)

- Ê†πÊçÆ$R$ ËÆ°ÁÆó/evaluate/compare Âπ∂ÊúÄÁªàÈÄâÊã© $\lambda$



### ‰∏çÂêåRegularisedÊñπÂºèÁöÑ Liner Regression ÂØπÊØî

![image-20181024154917930](assets/image-20181024154917930.png)





## Bias-variance trade-off

### ‰ªÄ‰πàÊòØBiasÂíåvariance

##### - Bias: Âú®Ê†∑Êú¨‰∏äÊãüÂêàÁöÑÂ•Ω‰∏çÂ•Ω

##### - Variance: Ê®°ÂûãÂú®ÊµãËØïÈõÜ‰∏äÁöÑË°®Áé∞Â•Ω‰∏çÂ•Ω

![image-20181024161159494](assets/image-20181024161159494.png)

- complexity model

  - Ë¥üË¥£ÁöÑmodelÈÄöÂ∏∏ÂèØ‰ª•ËÆ© training errorÂèòÂæóÂæàÂ∞è
  - ÁîöËá≥ÊòØ0 error(ÊúâÈôêsampleÁöÑÊó∂ÂÄô)

  |               |           |               |
  | ------------- | --------- | ------------- |
  | simple model  | High bias | Low variance  |
  | complex model | Low bias  | High variance |


### bias, varianceÂíåtest error‰ª•Âèäunderfit/overfitÁöÑÂÖ≥Á≥ª

|                                                              |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20181024161412052](assets/image-20181024161412052.png) | ![image-20181024161422765](assets/image-20181024161422765.png) |



## Perceptron (ÊÑüÁü•Êú∫, Á•ûÁªèÁΩëÁªúÂü∫Á°Ä)

- activation function
- feed forward networks

#### Convergence Theorem: 

```
if the training data is linearly separable, the algorithm is guaranteed to converge to a solution. That is, there exist a finite ùêæ such that L(w^K) = 0
```



#### Update $w$  in training

![image-20181024230809722](assets/image-20181024230809722.png) 



## Multilayer Perceptron 



#### Activation Funciton

![image-20181025194005547](assets/image-20181025194005547.png)



#### Loss funciton

![image-20181025194020289](assets/image-20181025194020289.png)

- differentiable
- no analytic solution (ÈÄöÂ∏∏Ê≤°ÊúâËß£ÊûêËß£)
- ‰ΩøÁî®stochastic gradient descentÁÆó(ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôç, ÊØèÊ¨°Êõ¥Êñ∞$\theta$Âè™Áî®‰∏Ä‰∏™sample) - ÂØπÊØîÊâπÈáèÊ¢ØÂ∫¶‰∏ãÈôç, ÊØèÊ¨°Êõ¥Êñ∞‰ΩøÁî®ÂÖ®ÈÉ®sample







## Backpropagation ( of errors)

- ‰ªéÊúÄÂêé‰∏ÄÂ±ÇÂêëÂâç(ÂÅèÂØº)
- Ê≠£ÂàôÂåñ(regularisation, implicit and explicit)





## Deep learning

- Any Boolean function over ùëö variables can be implemented using a hidden layer with up to 2Ùè∞é elements 
- **vanishing gradient problem**

> **ÂéüÂõ†Ôºö**ÂâçÈù¢Â±Ç‰∏äÁöÑÊ¢ØÂ∫¶ÊòØÊù•Ëá™‰∫éÂêéÈù¢Â±Ç‰∏äÊ¢ØÂ∫¶ÁöÑ‰πò‰πòÁßØ„ÄÇÂΩìÂ≠òÂú®ËøáÂ§öÁöÑÂ±ÇÊ¨°Êó∂ÔºåÂ∞±Âá∫Áé∞‰∫ÜÂÜÖÂú®Êú¨Ë¥®‰∏äÁöÑ‰∏çÁ®≥ÂÆöÂú∫ÊôØÔºåÂ¶ÇÊ¢ØÂ∫¶Ê∂àÂ§±ÂíåÊ¢ØÂ∫¶ÁàÜÁÇ∏„ÄÇ
>
> https://blog.csdn.net/cppjava_/article/details/68941436





### Convolutional Neural Networks(CNN)

### Components of a CNN

- Convolutional layers
  - complex input representations based on convolution operation
  - filter weights are learned from training data
- **Downsampling (usually via Max Pooling)**
  - re-scales to smaller resolution, limits parameter explosion
  - Max Pooling: 
    - $v = max(u_{11},u_{11},...,u_{mm})$ (ÈÄâÊã©Âç∑ÁßØ‰∏≠ÁöÑÊúÄÂ§ßvaluableÁöÑÈÇ£‰∏™)
    - invariance(‰∏çÂèòÊÄß)ÔºåËøôÁßç‰∏çÂèòÊÄßÂåÖÊã¨translation(Âπ≥Áßª)Ôºårotation(ÊóãËΩ¨)Ôºåscale(Â∞∫Â∫¶)
    - ‰øùÁïô‰∏ªË¶ÅÁöÑÁâπÂæÅÂêåÊó∂ÂáèÂ∞ëÂèÇÊï∞(ÈôçÁª¥ÔºåÊïàÊûúÁ±ª‰ººPCA)ÂíåËÆ°ÁÆóÈáèÔºåÈò≤Ê≠¢ËøáÊãüÂêàÔºåÊèêÈ´òÊ®°ÂûãÊ≥õÂåñËÉΩÂäõ
    - https://www.zhihu.com/question/36686900
- Fully connected parts and output layer
  - merges representations together



## Support Vector Machines

https://zhuanlan.zhihu.com/p/31258516

![image-20181026120509487](assets/image-20181026120509487.png)

### Difference between SVM and perceptron

- Perceptron: min perceptron loss as studied earlier
- SVM: different criterion for choosing parameters



### Maximum margin classifier

- perpendicular to(ÂûÇÁõ¥‰∫é)

- ÊúÄ‰ºòboundaryÊúâÊó†Èôê‰∏™(ÊúâÊ≠ß‰πâÁöÑ, ambiguity, ÂΩì **$w$** Âíå$b$ÈÉΩÊòØÊï¥Êï∞ÂÄç, ÂÆûÈôÖ‰∏äÊòØ‰∏ÄÊù°Á∫ø,‰ΩÜÊòØÊúâÊó†Êï∞ÁßçË°®Á§∫ÊñπÊ≥ï)

  - Ëß£ÂÜ≥ÊñπÊ°à1: measure the distance to the closest point, and rescale parameters(‰ªéËÄå‰ΩøËß£ÂèòÂæóÂîØ‰∏Ä):

  > **Steps:**
  >
  > ![image-20181026120550200](assets/image-20181026120550200.png)


### hard-margin SVM

![image-20181026120730050](assets/image-20181026120730050.png)



### Soft-margin SVM

3 approaches to address ***not-separable*** problem

- transform data (still use hard-margin svm)
- relax the constraints
- combination of above 2 approaches



#### Relax the contrains: allow points to be inside the margin or even on the wrong side of the boundary (‰ΩÜÊòØ‰ºöÂä†ÂÖ•‚ÄúÊÉ©ÁΩö‚ÄùÊú∫Âà∂)



| Hard Margin                                                  | Soft Margin                                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![ximage-20181026131313831](assets/image-20181026131313831.png) | ![image-20181026131332246](assets/image-20181026131332246.png) |
| ÂÖ∂ÂÆûÂ∞±ÊòØsoft margin‰∏≠Ê≠£ÂàôÂåñÂ∏∏Êï∞CÁöÑÂÄíÊï∞($\frac{1}{\lambda}$)Êó†ÈôêÂ§ß, ‰ΩøÂæóÂÖ∂Á¨¨‰∫åÈ°πÂØπÈîôËØØÂàÜÁ±ªÁöÑÂÆπÂøçÂ∫¶ÂèòÂæóÊûÅÂ∞è(Êó¢‰∏çÂÖÅËÆ∏ÈîôËØØ), ÂÆπÊòìÂá∫Áé∞**overfit** | CË∂äÂ∞è, ÂØπÈîôËØØÁöÑÂÆπÂøçÂ∫¶Ë∂äÂ§ß(Êó¢ÂÖÅËÆ∏ÈîôËØØ), ÂÆπÊòìÂá∫Áé∞**underfit    |
|                                                              |                                                              |



### Ëß£Ê≥ï

- Êó†Á∫¶ÊùüÁöÑ‰ºòÂåñÈóÆÈ¢ò: $min f(x)$
- Â∏¶Á≠âÂºèÁ∫¶ÊùüÁöÑ‰ºòÂåñÈóÆÈ¢ò:  $minf(x), s.t. h(x)=0$  (Lagrangian Duality)
- Â∏¶‰∏çÁ≠âÂºèÁ∫¶ÊùüÁöÑ‰ºòÂåñÈóÆÈ¢ò: $min f(x) , s.t.h(x) \leq 0$ (KKT )



### Ê†∏ÂáΩÊï∞ (for non-liner data, feature space transformation)

- Map data into a new feature space
- run hard-margin or soft-margin SVM in new space
- decision boundary is non-linear in original space



#### È´òÊñØÊ†∏ÂáΩÊï∞

- Âú∞Ê†á (landmarks)
- **Gaussian Kernel**
  - ËæÉÂ§ßÊó∂ÔºåÂèØËÉΩ‰ºöÂØºËá¥‰ΩéÊñπÂ∑ÆÔºåÈ´òÂÅèÂ∑Æ(bias, underfit)Ôºõ
  - ËæÉÂ∞èÊó∂ÔºåÂèØËÉΩ‰ºöÂØºËá¥‰ΩéÂÅèÂ∑ÆÔºåÈ´òÊñπÂ∑Æ(variance, overfit)

> ![image-20181026160456787](assets/image-20181026160456787.png)

- Â§öÈ°πÂºèÊ†∏ÂáΩÊï∞Ôºà**Polynomial Kerne**lÔºâ
- Â≠óÁ¨¶‰∏≤Ê†∏ÂáΩÊï∞Ôºà**String kernel**Ôºâ
- Âç°ÊñπÊ†∏ÂáΩÊï∞Ôºà **chi-square kernel**Ôºâ
- Áõ¥ÊñπÂõæ‰∫§ÈõÜÊ†∏ÂáΩÊï∞Ôºà**histogram intersection kernel**Ôºâ



### Logical regression vs SVM

$n$‰∏∫ÁâπÂæÅÊï∞Ôºå$m$‰∏∫ËÆ≠ÁªÉÊ†∑Êú¨Êï∞„ÄÇ

- $n > m$   ==> LR or SVM without kernel
- small n(1-1000) and not that big m(10-10000)  ==> SVM with Gaussian kernel
- small n and big m  ==> ÊâæÂà∞Êõ¥Â§öfeatures, Âπ∂‰ΩøÁî®Á¨¨‰∏ÄÁßç





### Representer theorem

‰∏Ä‰∏™Áî®Êù•ËØÜÂà´Êñ∞ÊñπÊ≥ï(learner)ÊòØ‰∏çÊòØÊúâÊïàÁöÑÊ†∏ÂáΩÊï∞ÁöÑtool

**(Tells us when a (decision‚Äêtheoretic) learner is kernelizable)**

- $f$ function is a reproducing kernel Hilbert space(RKHS)



## Constructing Kernels

### Polynomial kernel

![image-20181026171845522](assets/image-20181026171845522.png)

- Here ùíñ and ùíó are vectors with ùëö components 
- ùëë Ùè∞Ü 0isanintegerandùëê Ùè∞Ü 0isaconstant 



### Identifying new kernels

#### Method 01

![image-20181026172150552](assets/image-20181026172150552.png)



#### Method 02

Using **Mercer‚Äôs** theorem

using function $f$ to generate a matrix of $n * n$ size (each item = $f(a_i,a_j)$) and if the matrix is **positive-semidefinite**, then it is a valid kernel function.

![image-20181026173402324](assets/image-20181026173402324.png)







Example: ËØÅÊòéGaussian KernelÊòØ‰∏Ä‰∏™Ê†∏ÂáΩÊï∞



![image-20181026172857086](assets/image-20181026172857086.png)

> ÊúÄÂêé‰∏ÄÊ≠•ÁöÑËØ¥Êòé(Ê†πÊçÆmethod 01ÁöÑidentity):
>
> - ÁÇπ‰πòÊòØÊ†∏ÂáΩÊï∞(dÊ¨°ÊñπÁöÑdot product)
> - Ê†∏ÂáΩÊï∞‰πò‰ª•Â∏∏Êï∞($r_d$)ËøòÊòØÊ†∏ÂáΩÊï∞
> - Ê†∏ÂáΩÊï∞Áõ∏Âä†ËøòÊòØÊ†∏ÂáΩÊï∞
> - Ê†∏ÂáΩÊï∞ÂâçÂêé‰πò‰ª•$f(u), f(v)$‰ªçÁÑ∂ÊòØÊ†∏ÂáΩÊï∞
>
> ===> ÊâÄ‰ª•Êï¥‰∏™‰∏≠Èó¥È°πÊòØÊ†∏ÂáΩÊï∞





## Ensemble Methods (ÈõÜÊàêÂ≠¶‰π†)

###  Bagginng

- Construct "near-independent" datasets via sampling with replacement(ÊúâÊîæÂõûÁöÑÈáçÂ§çÊäΩÊ†∑, ÊØèÊ¨°ÂèñÊ†∑ÂêéÊ†∑ÂìÅ‰∏ç‰ºöË¢´drop, ‰ªçÁÑ∂Âú®ÂèñÊ†∑Á©∫Èó¥‰∏≠)
- sampling - training - predicting - evalucating 
- ÂèØÂπ∂Ë°å
- Ëß£ÂÜ≥high variance(overfit)ÁöÑÈóÆÈ¢ò



#### Sampling

| Original training set                                        | Samples (with replacement)                                   |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20181026191540608](assets/image-20181026191540608.png) | ![image-20181026191532759](assets/image-20181026191532759.png) |

#### Example: Random Forest (baggingÁöÑÂÖ∏ÂûãÁÆóÊ≥ï)

![image-20181026191937996](assets/image-20181026191937996.png)

> ÊØèÊ£µÊ†ëÁöÑfeaturesÈÉΩÊòØ‰∏çÂêåÁöÑ (‰πüÊúâËØ¥ÊØè‰∏™nodeÁöÑfeatureÈÉΩÊòØ‰∏çÂêåÁöÑ)

 ##### ‰ΩøÁî®out-of-sample dataËøõË°åÊµãËØï

> ÁêÜËÆ∫‰∏ä, ÂΩìÊ†∑Êú¨Ë∂≥Â§üÂ§öÁöÑÊó∂ÂÄô, ÊØèÊù°Ê†∑Êú¨ÈÉΩÊúâ$e^{-1}=0.368$ÁöÑÊ¶ÇÁéá‰∏çË¢´ÈÄâÂà∞.
>
> ËøôÈÉ®ÂàÜÊï∞ÊçÆÂè´ÂÅö out-of-sample data, ÂèØ‰ª•Áî®Êù•ÂÅöevaluate



### Boosting

- Ëß£ÂÜ≥high bias(underfit)ÁöÑÈóÆÈ¢ò
- ÊúâÊîæÂõûÁöÑÊäΩÊ†∑(‰ΩÜÊòØÂü∫‰∫éweightÈÄâÊã©, ‰∏çÊòØrandom)
- ÈÄÇÁî®‰∫éÂàÜÁ±ªÂô®ÊòØ‚Äú**Âº±(weak)ÂàÜÁ±ªÂô®**‚Äù(ÂÆπÊòìunderfit)
- Âü∫‰∫éËø≠‰ª£(iteration), ÊØèÊ¨°Ëø≠‰ª£ÈÄâÊã©Ê†∑Êú¨ÁöÑÊó∂ÂÄô,Âü∫‰∫é‰πãÂâçËø≠‰ª£ÁªìÊûúÂæóÂà∞ÁöÑ**ÊùÉÈáç(weight)**
- ËÆ°ÁÆóÈáèÊØîbaggingÂ§ß
- ÂÆπÊòìoverfit (ÊâÄ‰ª•Âè™ÈÄÇÁî®‰∫éÂº±ÂàÜÁ±ªÂô®, ‰πüÂ∞±ÊòØÂÆπÊòìunderfitÁöÑÂàÜÁ±ªÂô®; Â¶ÇÊûú‰ΩøÁî®performanceËæÉÂ•ΩÁöÑÂü∫ÂàÜÁ±ªÂô®Â∞±Êõ¥ÂÆπÊòìoverfit)





#### Sampling

| Original                                                     | Boosting (more likely to select the ones that are misclassified) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20181026213427538](assets/image-20181026213427538.png) | ![image-20181026213433486](assets/image-20181026213433486.png) |



#### Steps

![image-20181026213315303](assets/image-20181026213315303.png)


#### Example: AdaBoost

![image-20181026213556092](assets/image-20181026213556092.png)





#### Bagging vs Boosting

![image-20181026214050643](assets/image-20181026214050643.png)



#### Stacking

ÊúâÁÇπÂÉèANN, Ê†πÊçÆbase modelÁöÑoutputÂÜçÊ¨°ÁîüÊàêmeta-model,Á±ª‰ºº‰∫éÁªôÊØè‰∏™base model‰∏Ä‰∏™ÊùÉÈáç(linear regression)

 $meta\_model=f(base\_models)$

- ËÆ°ÁÆóÈáèÂ§ß (computationally expensive)
- ÂèØ‰ª•‰ΩøÁî®Â§öÁßçbase model



## Bandit

### MAB(Multi-armed bandit)

##### $\epsilon-Greedy $

- use $\epsilon $(0~1) to control exploration and exploitation
- trade-off between exploration and exploitation (using $\epsilon$) 
  - greedy($\epsilon=0$): increases fastest
  - high exploration (high $\epsilon$): increases faster
  - high exploitation (low $\epsilon$): increases slower, but eventually superior to high $\epsilon$.
- selection of initialisation value for Greedy ($\epsilon = 0$) (of estimate value Q)
  - Pessimism(ÊÇ≤ËßÇ): ‰ΩøÁî®ÊØîËßÇÂØüÂÄºÂ∞èÁöÑ --> Ê∞∏ËøúÂè™ÈÄâ‰∏Ä‰∏™arm 
  - Optimism(‰πêËßÇ): ‰ΩøÁî®ÊØîËßÇÂØüÂÄºÂ§ßÁöÑ --> Â∞ùËØïÊâÄÊúâarms



##### UCB (Upper Confidence Bandit)

UCB algorithm

|                                                              | Average Reward                                               | regretbounds                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20181027105112615](assets/image-20181027105112615.png) | ![image-20181027105120356](assets/image-20181027105120356.png) | ![image-20181027105131296](assets/image-20181027105131296.png) |
|                                                              | ![image-20181027105152717](assets/image-20181027105152717.png)<br />Ââçt-1ËΩÆÁöÑÂπ≥Âùáreward | ![image-20181027105238903](assets/image-20181027105238903.png)<br /><br />Ëøô‰∏ÄÊï¥È°π‰ª£Ë°®‰∏Ä‰∏™Âä†ÊùÉ, Ë¢´ÈÄâÊã©Ê¨°Êï∞Ë∂äÂ∞ëÁöÑarmËøô‰∏ÄÈ°πÂ∞±Ë∂äÂ§ß, ‰πüÂ∞±Ë∂äÊúâÂèØËÉΩË¢´explore.<br />‰πüÂ∞±ÊòØËØ¥Ëøô‰∏ÄÈ°πÊòØÁî®Êù•Âπ≥Ë°°explorationÂíåexploitationÁöÑ |

###### Flexible change

Using $p$ to replace $2$ in the second part for the reason of balancing the weight of **selecting existing best one** or **exploration**.

![image-20181027105639053](assets/image-20181027105639053.png)



## Unsupervised Learning

### Tasks

- Clustering
- Dimensionality reduction
- Learning parameters of probabilistic models

### Applications:

- ÂïÜÂìÅÂÖ≥ËÅîÊÄßÂàÜÊûê(Áâ©ÂìÅÊÄªÊòØË¢´‰∏ÄËµ∑‰π∞, Âï§ÈÖíÂ∞øÂ∏É)
- Â•áÂºÇÂÄºÊ£ÄÊµã(ËØàÈ™óËØÜÂà´, ‰ø°Áî®Âç°ÁõóÂà∑ËØÜÂà´)



### K-means clustering

![image-20181027110900276](assets/image-20181027110900276.png)



## Gaussian Mixture Model (GMM)

clusteringÈóÆÈ¢òËΩ¨Âåñ

![image-20181027115321582](assets/image-20181027115321582.png)

- Êó†Ê≥ïÊ±ÇÂØº
- log trickÂêéÊ±ÇÂØº‰πü‰∏ç‰ø°
- ÂÅèÂØº‰πü‰∏çË°å, Âõ†Ê≠§Ê¢ØÂ∫¶‰∏ãÈôç‰∏çË°å
- **Âè™ËÉΩÁî®Expectation Maximisation(EM)**



### Expectation Maximisation

| MLE                                                  | EM                                      |
| ---------------------------------------------------- | --------------------------------------- |
| a frequentist principle                              | an algorithm(Ê±ÇËß£MLEÁöÑ)                 |
| maximise the probability of the data                 | a way to solve the problem posed by MLE |
| can be solved by other methods like gradient descent |                                         |
|                                                      |                                         |

#### ‰∏∫‰ªÄ‰πàË¶ÅÁî®EMÁÆóÊ≥ï

1. Ê≤°ÊúâË∂≥Â§üÁöÑÂ∑≤Áü•ÈáèÊù•ËÆ°ÁÆó
2. ËÆ°ÁÆóÂæàÈ∫ªÁÉ¶



#### EMÁöÑ‰∏ªË¶ÅÊÄùÊÉ≥

- Â∞ÜÂèòÈáèÂàÜ‰∏∫$observed$  variablesÂíå*unobserved* variables (latent variables)

- Âä†ÂÖ•È¢ùÂ§ñÁöÑÂÜó‰ΩôÂèòÈáè(additional variables might seem redundant)

- maximising ‰∏ãÈôê

  ![image-20181027120922034](assets/image-20181027120922034.png)

> **key idea: ‰ΩøÁî®$\theta$ Êù•Ë°®Á§∫$p(Z)$**
>
> Âè≥ËæπÈÉ®ÂàÜÂ∞±ÊòØEMÁöÑ‰∏ãÈôê. ÊèêÈ´ò‰∏ãÈôê‰πüÂ∞±ÊòØÊèêÈ´òÂè≥ËæπÈÉ®ÂàÜÁöÑÂÄº.
>
> - 0. Âè≥ËæπÈÉ®ÂàÜÊòØ‰∏Ä‰∏™ÂÖ≥‰∫é**‰∏§‰∏™**ÂèòÈáèÁöÑÂáΩÊï∞, Êó†Ê≥ïÂêåÊó∂Ê±ÇËß£, Âõ†Ê≠§‰ΩøÁî®**ÊéßÂà∂ÂèòÈáè** (ÂÖàÂõ∫ÂÆö$\theta$ÁÆó$p(Z)$, Âú®Âõ∫ÂÆö$p(Z)$ÁÆó$\theta$).
>
> - 1. ÂΩìset $p(Z) = p(Z|X,\theta^*)$Êó∂, lower bound becomes tight 
> - 2. lower boundÁöÑÁ¨¨‰∫åÈ°πÊòØ‰∏é$\theta$Êó†ÂÖ≥ÁöÑ
> - 3. Êª°Ë∂≥step1,2, lower boundÂ∞±ÊòØ‰∏Ä‰∏™ÂÖ≥‰∫é$\theta$ÁöÑ‰∏ÄÂÖÉÂáΩÊï∞, Â∞±ÂèØ‰ª•Ê±ÇËß£ÊûêËß£‰∫Ü



### GMM‰∏≠ÁöÑEM

Step E (Expectation)



Step M (Maximisation)



#### responsibilities

![image-20181028105044102](assets/image-20181028105044102.png)



#### K-meansÊòØ‰∏ÄÁßçÁâπÊÆäÁöÑGMM(Êù°‰ª∂ÂèóÈôê)

k-meansÊòØÊØè‰∏™È´òÊñØÂàÜÂ∏ÉÁöÑÊùÉÈáçÈÉΩÊòØ$\frac{1}{k}$ ÁöÑGMM







## PCA

reduce count of dimensions via data transformation

![image-20181028123910018](assets/image-20181028123910018.png)



### Â∫îÁî®Âú∫ÊôØ

ÂÅá‰ΩøÊàë‰ª¨Ê≠£Âú®ÈíàÂØπ‰∏ÄÂº† 100√ó100ÂÉèÁ¥†ÁöÑÂõæÁâáËøõË°åÊüê‰∏™ËÆ°ÁÆóÊú∫ËßÜËßâÁöÑÊú∫Âô®Â≠¶‰π†ÔºåÂç≥ÊÄªÂÖ±Êúâ10000 ‰∏™ÁâπÂæÅ„ÄÇ

> 1. Á¨¨‰∏ÄÊ≠•ÊòØËøêÁî®‰∏ªË¶ÅÊàêÂàÜÂàÜÊûêÂ∞ÜÊï∞ÊçÆÂéãÁº©Ëá≥1000‰∏™ÁâπÂæÅ
> 2. ÁÑ∂ÂêéÂØπËÆ≠ÁªÉÈõÜËøêË°åÂ≠¶‰π†ÁÆóÊ≥ï
> 3. Âú®È¢ÑÊµãÊó∂ÔºåÈááÁî®‰πãÂâçÂ≠¶‰π†ËÄåÊù•ÁöÑÂ∞ÜËæìÂÖ•ÁöÑÁâπÂæÅËΩ¨Êç¢ÊàêÁâπÂæÅÂêëÈáèÔºåÁÑ∂ÂêéÂÜçËøõË°åÈ¢ÑÊµã
>
> Ê≥®ÔºöÂ¶ÇÊûúÊàë‰ª¨Êúâ‰∫§ÂèâÈ™åËØÅÈõÜÂêàÊµãËØïÈõÜÔºå‰πüÈááÁî®ÂØπËÆ≠ÁªÉÈõÜÂ≠¶‰π†ËÄåÊù•ÁöÑ„ÄÇ



#### ÈîôËØØÁî®Ê≥ï

- Áî®‰∫éÂáèÂ∞ëËøáÊãüÂêà
- Êª•Áî® (Â∞ùËØïÁÆóÊ≥ï‰πãÂâçÊÄªÊòØÂÖàËøõË°åPCA, ËøôÂèØËÉΩ‰ºö‰∏¢Êéâ‰∏Ä‰∫õÈáçË¶ÅÁöÑfeature)



## BayesianRegression

1. ÂÅáËÆæÂÖàÈ™åÊ¶ÇÁéá prior
2. ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂæóÂà∞ÂêéÈ™åÊ¶ÇÁéá posterior



















