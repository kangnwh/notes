# 算法





| ML Methods                         | Concepts                                                     | Problems                                                     | Solutions                                                    |
| ---------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Linear Regression                  | Lost function(L1, L2)<br />                                  | - Minimise (squared) errors<br />- MLE                       | Analytic: Normal Equation<br />Approxi: Gradient Descent     |
| Logical Regression                 | Logistic function<br />Sigmoid function<br />log trick<br />transform data(kernel function or ANN)<br />Polynomial regression<br />radial basis functions (RBFs) | - MLE<br />- Kernelise*                                      | Analytic: Normal Equation<br />Approxi: Gradient Descent     |
| Regularisation                     | To solve:<br />    - Irrelevant features<br />    - ill-posed problem<br /><br />Regulariser as prior<br />Constrained modelling<br />Bias-variance trade-off<br /> | - Add $\lambda $ for regularisation<br /><br />   =><br /> ridge regression(L2)<br />   => lasso regression(L1),can assign zero weight to some features, which is good if there are too many features<br />- MAP<br /> | Use heldout/cross validation to choose $\lambda$ (try and try...) |
| Perceptron                         | Activation function<br />- step<br />- sign<br />- logical (sigmoid)<br />- ReLU | Stochastic Gradient Descent(batch)<br /><br />**Cons**: 现行不可分时无解; 总是有(无限)多个解; | 如果分类错了, 那么<br />$𝑤􏰆←𝑤􏰆􏰎_i􏰆􏰈  - \eta * sign(s)*x_i$   |
| Backpropagation                    | Chain Rule<br />Generative/Discriminative Mode               | Stochastic Gradient Descent(batch size= 1)<br />**Cons**:容易overfit;early stopping<br />**Solutions**: regularisation |                                                              |
| Convolutional Neural Network (CNN) | Depth(hidden layers count) vs Width(nodes count)<br />**vanishing gradient problem**<br />sliding window<br />Downsampling<br />**Autoencoder**(used for setup, can **reduce dimensions** or **compress data**) | **Max pooling**(for downsamping, 选择最能表现数据的一个点留下)<br />reuse weights in the first layer |                                                              |
| Hard-margin SVM                    | 严格按照support vectors分                                    | - ![image-20181030165832755](assets/image-20181030165832755.png)<br />- 解的无限多种表示<br />- loss function=错误(包括在boundary以内)就是$\infin$, 正确就是$0$ | ![image-20181030170509023](assets/image-20181030170509023.png) |
| Soft-margin SVM                    | 相比hard-margin, 允许一些错误(包括在boundary之内这种错误)    | - relax constraints<br />- error penalise 少一点             | ![image-20181030170519978](assets/image-20181030170519978.png)<br />- Lagrangian Duality(拉格朗日乘子法,条件受限时的最优解问题) |
| Kernel Method                      | - Separating “learning module” from feature transformation<br />- Constructing kernels<br />   \|- kernel 协方差矩阵是半正定矩阵(positive semi-definite, 对称的)<br />    \|- polynomial kernel ($K(u,v) = (u'v+c)^d$)<br />- Representer theorem | - complex if use transform function for each instance's feature<br /> | - use kernel function directly (with all features passed to the function)<br />- Identifying new kernels<br />     \|-(半正定矩阵):<br />![image-20181030171708468](assets/image-20181030171708468.png) <br />     \|- Mercer’s theorem<br />![image-20181030171834486](assets/image-20181030171834486.png) |
| Ensemble                           | - Bagging(sampling with replacement, with identical weights for all instances )<br />- Boosting(samping with replacement, with higher weight for error instances) | - Bagging: 可以并行(parallel)<br />- Boosting: 只能串行(sequential); 容易overfit,所以base model只能是weak的<br />- stack: 使用base model的结果,训练出来一个meta model<br />![image-20181030172504754](assets/image-20181030172504754.png) | - Bagging: voting<br />     \|- example: random forest:![image-20181030172233063](assets/image-20181030172233063.png)<br />- Boosting: weighted voting |
| Bandits                            | - greedy<br />- $\epsilon-Greedy$<br />- UCB                 | - $\epsilon-Greedy$在随机选取的时候,没有使用现有知识<br />- UCB使用公式(公式中考虑了explored arm的收益,以及unexplored arm) |                                                              |
| GMM and EM                         | - EM是解MLE的一种方法(当存在latent variable,或者解析解不好求的时候) |                                                              | - EM: 2步, Expectation and Maximisation(控制变量)<br />- 当GMM中所有的Gaussian都有相同的$w_c$和$\delta^2$ 的时候, 就是K-means |
| PCA(principle component analysis)  | - Dimensionality reduction<br />     \|- Visualisation<br />     \|- Computational efficiency<br />     \|- Data compression | - covariance matrix<br />![image-20181030174730742](assets/image-20181030174730742.png)<br />- kernel PCA(transform then run PCA) | 1. Choose a direction<br />2. Choose the next direction<br />3. Repeat 2<br /><br />4. Project original data on the new axes<br />5. For each point, keep only the first 𝑙 coordinates |
| Bayesian Regression                | - $\theta$ 的概率分布                                        | - Sequential Bayesian Updating                               | ![image-20181030212641717](assets/image-20181030212641717.png) |
| Bayesian Classificaiton            |                                                              |                                                              |                                                              |
| PGM                                |                                                              |                                                              |                                                              |

