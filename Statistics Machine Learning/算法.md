# ç®—æ³•





| ML Methods                         | Concepts                                                     | Problems                                                     | Solutions                                                    |
| ---------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Linear Regression                  | Lost function(L1, L2)<br />                                  | - Minimise (squared) errors<br />- MLE                       | Analytic: Normal Equation<br />Approxi: Gradient Descent     |
| Logical Regression                 | Logistic function<br />Sigmoid function<br />log trick<br />transform data(kernel function or ANN)<br />Polynomial regression<br />radial basis functions (RBFs) | - MLE<br />- Kernelise*                                      | Analytic: Normal Equation<br />Approxi: Gradient Descent     |
| Regularisation                     | To solve:<br />    - Irrelevant features<br />    - ill-posed problem<br /><br />Regulariser as prior<br />Constrained modelling<br />Bias-variance trade-off<br /> | - Add $\lambda $ for regularisation<br /><br />   =><br /> ridge regression(L2)<br />   => lasso regression(L1),can assign zero weight to some features, which is good if there are too many features<br />- MAP<br /> | Use heldout/cross validation to choose $\lambda$ (try and try...) |
| Perceptron                         | Activation function<br />- step<br />- sign<br />- logical (sigmoid)<br />- ReLU | Stochastic Gradient Descent(batch)<br /><br />**Cons**: ç°è¡Œä¸å¯åˆ†æ—¶æ— è§£; æ€»æ˜¯æœ‰(æ— é™)å¤šä¸ªè§£; | å¦‚æœåˆ†ç±»é”™äº†, é‚£ä¹ˆ<br />$ğ‘¤ô°†â†ğ‘¤ô°†ô°_iô°†ô°ˆ  - \eta * sign(s)*x_i$   |
| Backpropagation                    | Chain Rule<br />Generative/Discriminative Mode               | Stochastic Gradient Descent(batch size= 1)<br />**Cons**:å®¹æ˜“overfit;early stopping<br />**Solutions**: regularisation |                                                              |
| Convolutional Neural Network (CNN) | Depth(hidden layers count) vs Width(nodes count)<br />**vanishing gradient problem**<br />sliding window<br />Downsampling<br />**Autoencoder**(used for setup, can **reduce dimensions** or **compress data**) | **Max pooling**(for downsamping, é€‰æ‹©æœ€èƒ½è¡¨ç°æ•°æ®çš„ä¸€ä¸ªç‚¹ç•™ä¸‹)<br />reuse weights in the first layer |                                                              |
| Hard-margin SVM                    | ä¸¥æ ¼æŒ‰ç…§support vectorsåˆ†                                    | - ![image-20181030165832755](assets/image-20181030165832755.png)<br />- è§£çš„æ— é™å¤šç§è¡¨ç¤º<br />- loss function=é”™è¯¯(åŒ…æ‹¬åœ¨boundaryä»¥å†…)å°±æ˜¯$\infin$, æ­£ç¡®å°±æ˜¯$0$ | ![image-20181030170509023](assets/image-20181030170509023.png) |
| Soft-margin SVM                    | ç›¸æ¯”hard-margin, å…è®¸ä¸€äº›é”™è¯¯(åŒ…æ‹¬åœ¨boundaryä¹‹å†…è¿™ç§é”™è¯¯)    | - relax constraints<br />- error penalise å°‘ä¸€ç‚¹             | ![image-20181030170519978](assets/image-20181030170519978.png)<br />- Lagrangian Duality(æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•,æ¡ä»¶å—é™æ—¶çš„æœ€ä¼˜è§£é—®é¢˜) |
| Kernel Method                      | - Separating â€œlearning moduleâ€ from feature transformation<br />- Constructing kernels<br />   \|- kernel åæ–¹å·®çŸ©é˜µæ˜¯åŠæ­£å®šçŸ©é˜µ(positive semi-definite, å¯¹ç§°çš„)<br />    \|- polynomial kernel ($K(u,v) = (u'v+c)^d$)<br />- Representer theorem | - complex if use transform function for each instance's feature<br /> | - use kernel function directly (with all features passed to the function)<br />- Identifying new kernels<br />     \|-(åŠæ­£å®šçŸ©é˜µ):<br />![image-20181030171708468](assets/image-20181030171708468.png) <br />     \|- Mercerâ€™s theorem<br />![image-20181030171834486](assets/image-20181030171834486.png) |
| Ensemble                           | - Bagging(sampling with replacement, with identical weights for all instances )<br />- Boosting(samping with replacement, with higher weight for error instances) | - Bagging: å¯ä»¥å¹¶è¡Œ(parallel)<br />- Boosting: åªèƒ½ä¸²è¡Œ(sequential); å®¹æ˜“overfit,æ‰€ä»¥base modelåªèƒ½æ˜¯weakçš„<br />- stack: ä½¿ç”¨base modelçš„ç»“æœ,è®­ç»ƒå‡ºæ¥ä¸€ä¸ªmeta model<br />![image-20181030172504754](assets/image-20181030172504754.png) | - Bagging: voting<br />     \|- example: random forest:![image-20181030172233063](assets/image-20181030172233063.png)<br />- Boosting: weighted voting |
| Bandits                            | - greedy<br />- $\epsilon-Greedy$<br />- UCB                 | - $\epsilon-Greedy$åœ¨éšæœºé€‰å–çš„æ—¶å€™,æ²¡æœ‰ä½¿ç”¨ç°æœ‰çŸ¥è¯†<br />- UCBä½¿ç”¨å…¬å¼(å…¬å¼ä¸­è€ƒè™‘äº†explored armçš„æ”¶ç›Š,ä»¥åŠunexplored arm) |                                                              |
| GMM and EM                         | - EMæ˜¯è§£MLEçš„ä¸€ç§æ–¹æ³•(å½“å­˜åœ¨latent variable,æˆ–è€…è§£æè§£ä¸å¥½æ±‚çš„æ—¶å€™) |                                                              | - EM: 2æ­¥, Expectation and Maximisation(æ§åˆ¶å˜é‡)<br />- å½“GMMä¸­æ‰€æœ‰çš„Gaussianéƒ½æœ‰ç›¸åŒçš„$w_c$å’Œ$\delta^2$ çš„æ—¶å€™, å°±æ˜¯K-means |
| PCA(principle component analysis)  | - Dimensionality reduction<br />     \|- Visualisation<br />     \|- Computational efficiency<br />     \|- Data compression | - covariance matrix<br />![image-20181030174730742](assets/image-20181030174730742.png)<br />- kernel PCA(transform then run PCA) | 1. Choose a direction<br />2. Choose the next direction<br />3. Repeat 2<br /><br />4. Project original data on the new axes<br />5. For each point, keep only the first ğ‘™ coordinates |
| Bayesian Regression                | - $\theta$ çš„æ¦‚ç‡åˆ†å¸ƒ                                        | - Sequential Bayesian Updating                               | ![image-20181030212641717](assets/image-20181030212641717.png) |
| Bayesian Classificaiton            |                                                              |                                                              |                                                              |
| PGM                                |                                                              |                                                              |                                                              |

